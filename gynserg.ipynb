{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307d8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¥ Gynecological Surgery Organ Segmentation with SegFormer\n",
    "\n",
    "This notebook trains a SegFormer model to segment organs (uterus, ovary, fallopian tube) from laparoscopic surgery videos.\n",
    "\n",
    "**Model**: SegFormer-B0 (Hugging Face)  \n",
    "**Dataset**: BlackWalkersAnatomy from Kaggle  \n",
    "**Classes**: Background, Uterus, Ovary, Fallopian Tube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67389f",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies (Locked Versions)\n",
    "\n",
    "âš ï¸ **CRITICAL**: After running this cell, restart the kernel before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08820003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies with compatible versions...\")\n",
    "\n",
    "!pip install -q --no-cache-dir \\\n",
    "    \"numpy==1.26.4\" \\\n",
    "    \"transformers==4.44.0\" \\\n",
    "    \"datasets==2.14.0\" \\\n",
    "    \"albumentations\" \\\n",
    "    \"evaluate\" \\\n",
    "    \"Pillow\"\n",
    "\n",
    "print(\"\\nâœ… Installation complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”´ CRITICAL: RESTART KERNEL NOW\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nColab: Runtime â†’ Restart runtime\")\n",
    "print(\"Kaggle: Click restart button (âŸ³)\")\n",
    "print(\"\\nAfter restart:\")\n",
    "print(\"  â€¢ Skip this cell\")\n",
    "print(\"  â€¢ Run from Cell 2 onwards\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f341a",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    SegformerForSemanticSegmentation, \n",
    "    SegformerImageProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49f13c",
   "metadata": {},
   "source": [
    "## 3. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32934c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "BASE_PATH = \"/kaggle/input/blackwalkersanatomy/GynSurg_Anatomy_Dataset\"\n",
    "IMAGE_BASE = os.path.join(BASE_PATH, \"ganseg\")\n",
    "MASK_BASE = os.path.join(BASE_PATH, \"ganseg_mask\")\n",
    "\n",
    "# Class mapping for gynecological organs\n",
    "id2label = {\n",
    "    0: \"background\",\n",
    "    1: \"uterus\",\n",
    "    2: \"fallopian_tube\",\n",
    "    3: \"ovary\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_classes = len(id2label)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class mapping:\")\n",
    "for class_id, label in id2label.items():\n",
    "    print(f\"  Class {class_id}: {label}\")\n",
    "\n",
    "# Global variable for intensity mapping (will be set after data exploration)\n",
    "INTENSITY_TO_CLASS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b15d1b",
   "metadata": {},
   "source": [
    "## 4. Collect Image-Mask Pairs\n",
    "\n",
    "This section collects all image and mask file pairs from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548eed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corresponding_frame(mask_path, image_base_dir):\n",
    "    \"\"\"\n",
    "    Find the corresponding frame image for a mask\n",
    "    \n",
    "    mask_path: /path/to/ganseg_mask/GANSEG_01/0.mp4_/0_010800_06-00-00_mask.png\n",
    "    Returns: /path/to/ganseg/GANSEG_01/0.mp4_/0_010800_06-00-00.png (or .jpg)\n",
    "    \"\"\"\n",
    "    mask_filename = os.path.basename(mask_path)\n",
    "    \n",
    "    # Get the directory structure\n",
    "    parts = Path(mask_path).parts\n",
    "    ganseg_idx = parts.index('ganseg_mask')\n",
    "    ganseg_id = parts[ganseg_idx + 1]  # e.g., GANSEG_01\n",
    "    video_folder = parts[ganseg_idx + 2]  # e.g., 0.mp4_\n",
    "    \n",
    "    # Construct image directory\n",
    "    image_dir = os.path.join(image_base_dir, ganseg_id, video_folder)\n",
    "    \n",
    "    # Remove '_mask' from filename to get image filename\n",
    "    image_filename = mask_filename.replace('_mask.png', '.png')\n",
    "    image_path = os.path.join(image_dir, image_filename)\n",
    "    \n",
    "    # Try different extensions if needed\n",
    "    if not os.path.exists(image_path):\n",
    "        image_path = image_path.replace('.png', '.jpg')\n",
    "    if not os.path.exists(image_path):\n",
    "        image_path = image_path.replace('.jpg', '.jpeg')\n",
    "    \n",
    "    return image_path if os.path.exists(image_path) else None\n",
    "\n",
    "# Collect all mask files\n",
    "print(\"Collecting mask files from all GANSEG folders...\")\n",
    "all_mask_paths = []\n",
    "\n",
    "for ganseg_dir in sorted(os.listdir(MASK_BASE)):\n",
    "    ganseg_path = os.path.join(MASK_BASE, ganseg_dir)\n",
    "    if not os.path.isdir(ganseg_path):\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Processing {ganseg_dir}...\", end=\" \")\n",
    "    folder_mask_count = 0\n",
    "    \n",
    "    for video_folder in sorted(os.listdir(ganseg_path)):\n",
    "        video_path = os.path.join(ganseg_path, video_folder)\n",
    "        if not os.path.isdir(video_path):\n",
    "            continue\n",
    "        \n",
    "        # Get all mask files in this video folder\n",
    "        mask_files = glob.glob(os.path.join(video_path, '*_mask.png'))\n",
    "        all_mask_paths.extend(mask_files)\n",
    "        folder_mask_count += len(mask_files)\n",
    "    \n",
    "    print(f\"{folder_mask_count} masks found\")\n",
    "\n",
    "print(f\"\\nTotal mask files found: {len(all_mask_paths)}\")\n",
    "\n",
    "# Find corresponding images\n",
    "print(\"\\nMatching images to masks...\")\n",
    "image_paths = []\n",
    "mask_paths = []\n",
    "missing_count = 0\n",
    "\n",
    "for mask_path in tqdm(all_mask_paths):\n",
    "    image_path = find_corresponding_frame(mask_path, IMAGE_BASE)\n",
    "    if image_path:\n",
    "        image_paths.append(image_path)\n",
    "        mask_paths.append(mask_path)\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(f\"\\nâœ“ Successfully matched {len(image_paths)} image-mask pairs\")\n",
    "if missing_count > 0:\n",
    "    print(f\"âœ— {missing_count} masks without corresponding images\")\n",
    "\n",
    "# Show sample paths\n",
    "if len(image_paths) > 0:\n",
    "    print(f\"\\nSample paths:\")\n",
    "    print(f\"  Image: {image_paths[0]}\")\n",
    "    print(f\"  Mask:  {mask_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c02bb",
   "metadata": {},
   "source": [
    "## 5. Analyze Mask Intensities\n",
    "\n",
    "Detect all unique intensity values in the masks to create the intensity-to-class mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No image-mask pairs found! Check dataset structure.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: ANALYZING MASK INTENSITIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze multiple masks to get consistent intensity values\n",
    "print(\"\\nAnalyzing first 10 masks to detect all intensity values...\")\n",
    "all_intensities = set()\n",
    "\n",
    "for mask_path in mask_paths[:10]:\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "    if len(mask.shape) == 3:\n",
    "        mask = mask[:,:,0]  # Take first channel if RGB\n",
    "    all_intensities.update(np.unique(mask))\n",
    "\n",
    "all_intensities = sorted(list(all_intensities))\n",
    "print(f\"âœ“ Found {len(all_intensities)} unique intensity values: {all_intensities}\")\n",
    "\n",
    "# Create intensity to class mapping\n",
    "INTENSITY_TO_CLASS = {intensity: idx for idx, intensity in enumerate(all_intensities)}\n",
    "print(f\"\\nIntensity â†’ Class ID mapping:\")\n",
    "for intensity, class_id in INTENSITY_TO_CLASS.items():\n",
    "    organ_name = id2label.get(class_id, \"unknown\")\n",
    "    print(f\"  Intensity {intensity:3d} â†’ Class {class_id} ({organ_name})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4286a9",
   "metadata": {},
   "source": [
    "## 6. Random Sample Visualization\n",
    "\n",
    "Visualize a random sample with custom color mapping for each intensity class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: RANDOM SAMPLE VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# All available intensities you expect\n",
    "EXPECTED_CLASSES = {0, 85, 170, 255}\n",
    "\n",
    "# ---- CUSTOM COLOR MAP FOR YOUR CLASSES ----\n",
    "# Format: intensity_value : (R, G, B)\n",
    "COLOR_MAP = {\n",
    "    0:   np.array([255,   0,   0], dtype=np.uint8),  # background -> red\n",
    "    85:  np.array([  0,   0, 255], dtype=np.uint8),  # uterus     -> blue\n",
    "    170: np.array([  0, 255,   0], dtype=np.uint8),  # tube       -> green\n",
    "    255: np.array([160,  32, 240], dtype=np.uint8),  # ovary      -> purple\n",
    "}\n",
    "\n",
    "# Try up to 50 random samples to find one with max classes\n",
    "max_intensities = 0\n",
    "best_mask_idx = None\n",
    "\n",
    "for attempt in range(50):\n",
    "    idx = random.randint(0, len(mask_paths) - 1)  # random index\n",
    "    mask = np.array(Image.open(mask_paths[idx]))\n",
    "\n",
    "    if len(mask.shape) == 3:\n",
    "        mask = mask[:, :, 0]\n",
    "\n",
    "    vals = set(np.unique(mask))\n",
    "    if len(vals) > max_intensities:\n",
    "        max_intensities = len(vals)\n",
    "        best_mask_idx = idx\n",
    "\n",
    "    # if all classes appear, stop early\n",
    "    if EXPECTED_CLASSES.issubset(vals):\n",
    "        print(f\"Found full-class sample on attempt {attempt+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nSelected sample index: {best_mask_idx}\")\n",
    "sample_img_path = image_paths[best_mask_idx]\n",
    "sample_mask_path = mask_paths[best_mask_idx]\n",
    "\n",
    "print(f\"Randomly chosen image: {os.path.basename(sample_img_path)}\")\n",
    "print(f\"Randomly chosen mask:  {os.path.basename(sample_mask_path)}\\n\")\n",
    "\n",
    "img = Image.open(sample_img_path).convert('RGB')\n",
    "mask = Image.open(sample_mask_path)\n",
    "mask_array = np.array(mask)\n",
    "\n",
    "# Handle RGB mask\n",
    "if len(mask_array.shape) == 3:\n",
    "    mask_array = mask_array[:, :, 0]\n",
    "    print(\"Converted RGB mask â†’ grayscale channel\\n\")\n",
    "\n",
    "unique_values = np.unique(mask_array)\n",
    "print(f\"Intensities in this sample: {unique_values}\\n\")\n",
    "\n",
    "# Pixel distribution\n",
    "print(\"Pixel distribution:\")\n",
    "for val in unique_values:\n",
    "    count = np.sum(mask_array == val)\n",
    "    percentage = (count / mask_array.size) * 100\n",
    "    print(f\"  Intensity {val:3d}: {count:7d} pixels ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- VISUALIZATION ----------------\n",
    "n_intensities = len(unique_values)\n",
    "fig = plt.figure(figsize=(20, 4 * ((n_intensities + 2) // 3)))\n",
    "\n",
    "# Original image\n",
    "ax1 = plt.subplot(((n_intensities + 2) // 3) + 1, 3, 1)\n",
    "ax1.imshow(img)\n",
    "ax1.set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Full mask view with *manual* colors\n",
    "ax2 = plt.subplot(((n_intensities + 2) // 3) + 1, 3, 2)\n",
    "h, w = mask_array.shape\n",
    "color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "for val in unique_values:\n",
    "    color = COLOR_MAP.get(int(val), np.array([0, 0, 0], dtype=np.uint8))\n",
    "    color_mask[mask_array == val] = color\n",
    "\n",
    "ax2.imshow(color_mask)\n",
    "ax2.set_title(\"Full Mask (All Classes)\", fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Overlay view using the same COLOR_MAP\n",
    "ax3 = plt.subplot(((n_intensities + 2) // 3) + 1, 3, 3)\n",
    "img_array = np.array(img)\n",
    "overlay = img_array.copy()\n",
    "for val in unique_values:\n",
    "    if val == 0:\n",
    "        continue\n",
    "    mask_bool = mask_array == val\n",
    "    if not np.any(mask_bool):\n",
    "        continue\n",
    "    color_rgb = COLOR_MAP.get(int(val), np.array([0, 0, 0], dtype=np.uint8))\n",
    "    overlay[mask_bool] = (overlay[mask_bool] * 0.4 + color_rgb * 0.6).astype(np.uint8)\n",
    "\n",
    "ax3.imshow(overlay)\n",
    "ax3.set_title(\"Overlay on Image\", fontsize=14, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Individual class visualizations\n",
    "for idx, intensity_val in enumerate(unique_values):\n",
    "    ax = plt.subplot(((n_intensities + 2) // 3) + 1, 3, idx + 4)\n",
    "\n",
    "    colored_img = img_array.copy()\n",
    "    mask_bool = mask_array == intensity_val\n",
    "\n",
    "    if np.any(mask_bool):\n",
    "        color_rgb = COLOR_MAP.get(int(intensity_val), np.array([0, 0, 0], dtype=np.uint8))\n",
    "        colored_img[mask_bool] = (colored_img[mask_bool] * 0.3 + color_rgb * 0.7).astype(np.uint8)\n",
    "\n",
    "    ax.imshow(colored_img)\n",
    "    pixel_count = np.sum(mask_array == intensity_val)\n",
    "    perc = (pixel_count / mask_array.size) * 100\n",
    "\n",
    "    title = f\"Intensity = {intensity_val}\\n({pixel_count} px, {perc:.1f}%)\"\n",
    "    if intensity_val == 0:\n",
    "        title += \"\\n[BACKGROUND]\"\n",
    "\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ‘† RANDOM VISUALIZATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a508e",
   "metadata": {},
   "source": [
    "## 7. Prepare Train-Validation Split\n",
    "\n",
    "Split the dataset by video groups to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARING TRAIN-VALIDATION SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Group by GANSEG folder and video to avoid data leakage\n",
    "video_groups = []\n",
    "for mask_path in mask_paths:\n",
    "    parts = Path(mask_path).parts\n",
    "    ganseg_id = parts[parts.index('ganseg_mask') + 1]\n",
    "    video_folder = parts[parts.index('ganseg_mask') + 2]\n",
    "    video_groups.append(f\"{ganseg_id}_{video_folder}\")\n",
    "\n",
    "# Convert to dataframe for easier splitting\n",
    "df = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'mask_path': mask_paths,\n",
    "    'video_group': video_groups\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Total samples: {len(df)}\")\n",
    "print(f\"  Unique video groups: {df['video_group'].nunique()}\")\n",
    "\n",
    "print(f\"\\nSamples per video group:\")\n",
    "group_counts = df['video_group'].value_counts()\n",
    "for group, count in group_counts.items():\n",
    "    print(f\"  {group}: {count} frames\")\n",
    "\n",
    "# Split by video group to avoid leakage\n",
    "unique_groups = df['video_group'].unique()\n",
    "train_groups, val_groups = train_test_split(\n",
    "    unique_groups, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df['video_group'].isin(train_groups)]\n",
    "val_df = df[df['video_group'].isin(val_groups)]\n",
    "\n",
    "train_images = train_df['image_path'].tolist()\n",
    "train_masks = train_df['mask_path'].tolist()\n",
    "val_images = val_df['image_path'].tolist()\n",
    "val_masks = val_df['mask_path'].tolist()\n",
    "\n",
    "print(f\"\\nâœ“ Split complete:\")\n",
    "print(f\"  Training samples: {len(train_images)} ({len(train_images)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Validation samples: {len(val_images)} ({len(val_images)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTraining video groups: {sorted(train_groups)}\")\n",
    "print(f\"Validation video groups: {sorted(val_groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496611b1",
   "metadata": {},
   "source": [
    "## 8. Dataset Class\n",
    "\n",
    "Custom PyTorch Dataset for loading and preprocessing images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global mapping: raw mask intensities â†’ class IDs\n",
    "# 0=bg, 85=uterus, 170=tube, 255=ovary\n",
    "INTENSITY_TO_CLASS = {\n",
    "    0:   0,  # background\n",
    "    85:  1,  # uterus\n",
    "    170: 2,  # fallopian_tube\n",
    "    255: 3,  # ovary\n",
    "}\n",
    "\n",
    "class GynSurgDataset(Dataset):\n",
    "    \"\"\"Dataset for gynecological surgery segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, processor, \n",
    "                 target_size=512, augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.processor = processor\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Augmentation pipeline for training\n",
    "        if augment:\n",
    "            self.transform = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.3),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, \n",
    "                                   rotate_limit=15, p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, \n",
    "                                     val_shift_limit=10, p=0.3),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = None\n",
    "    \n",
    "    def mask_to_class_ids(self, mask):\n",
    "        \"\"\"\n",
    "        Convert grayscale mask intensities (0,85,170,255) to class IDs (0,1,2,3).\n",
    "        Uses the global INTENSITY_TO_CLASS mapping.\n",
    "        \"\"\"\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        # Handle RGB masks by taking first channel\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "        \n",
    "        # IMPORTANT: Keep original size, don't resize here\n",
    "        h, w = mask.shape\n",
    "        class_mask = np.zeros((h, w), dtype=np.int64)\n",
    "        \n",
    "        # Map each intensity to its class ID\n",
    "        for intensity, class_id in INTENSITY_TO_CLASS.items():\n",
    "            class_mask[mask == intensity] = class_id\n",
    "        \n",
    "        return class_mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Load and convert mask - KEEP ORIGINAL SIZE\n",
    "        mask = Image.open(self.mask_paths[idx])\n",
    "        mask = self.mask_to_class_ids(mask)\n",
    "        \n",
    "        # Apply augmentation (both image and mask at same size)\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        \n",
    "        # Process image with SegFormer processor (this resizes to target_size)\n",
    "        encoded = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Resize mask to match processor's target size using nearest neighbor\n",
    "        mask_tensor = torch.from_numpy(mask.astype(np.int64))\n",
    "        mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0).float()  # (1, 1, H, W)\n",
    "        \n",
    "        mask_resized = F.interpolate(\n",
    "            mask_tensor, \n",
    "            size=(self.target_size, self.target_size), \n",
    "            mode=\"nearest\"\n",
    "        )\n",
    "        mask_resized = mask_resized.squeeze(0).squeeze(0).long()  # (H, W)\n",
    "        \n",
    "        # Prepare output\n",
    "        encoded_inputs = {k: v.squeeze(0) for k, v in encoded.items()}\n",
    "        encoded_inputs[\"labels\"] = mask_resized\n",
    "        \n",
    "        return encoded_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949abcaf",
   "metadata": {},
   "source": [
    "## 9. Initialize Model and Processor\n",
    "\n",
    "Load the SegFormer model and image processor from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "# For better accuracy, try: \"nvidia/segformer-b1-finetuned-ade-512-512\"\n",
    "\n",
    "print(\"Loading SegFormer model and processor...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "\n",
    "# Initialize processor\n",
    "processor = SegformerImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Initialize model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nâœ“ Model loaded successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf96721",
   "metadata": {},
   "source": [
    "## 10. Create Datasets and DataLoaders\n",
    "\n",
    "Create training and validation datasets with augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = 512\n",
    "num_classes = 4  # 0:bg, 1:uterus, 2:tube, 3:ovary\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "train_dataset = GynSurgDataset(\n",
    "    train_images, \n",
    "    train_masks, \n",
    "    processor,\n",
    "    target_size=TARGET_SIZE,\n",
    "    augment=True  # Enable augmentation for training\n",
    ")\n",
    "\n",
    "val_dataset = GynSurgDataset(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    processor,\n",
    "    target_size=TARGET_SIZE,\n",
    "    augment=False  # No augmentation for validation\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Datasets created:\")\n",
    "print(f\"  Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"  Val dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# ------------------ TEST ONE RANDOM SAMPLE ------------------\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import random\n",
    "\n",
    "# Class IDs: 0=bg, 1=uterus, 2=tube, 3=ovary\n",
    "# Colors: bg=red, uterus=blue, tube=green, ovary=purple\n",
    "CLASS_COLORS = np.array([\n",
    "    [1.0, 0.0, 0.0],   # 0 - background (red)\n",
    "    [0.0, 0.0, 1.0],   # 1 - uterus (blue)\n",
    "    [0.0, 1.0, 0.0],   # 2 - fallopian_tube (green)\n",
    "    [0.63, 0.13, 0.94] # 3 - ovary (purple)\n",
    "])\n",
    "class_cmap = ListedColormap(CLASS_COLORS)\n",
    "\n",
    "print(\"\\nTesting dataset loading on a random sample...\")\n",
    "\n",
    "rand_idx = random.randint(0, len(train_dataset) - 1)\n",
    "sample = train_dataset[rand_idx]\n",
    "print(f\"  Random sample index: {rand_idx}\")\n",
    "print(f\"  pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"  labels shape: {sample['labels'].shape}\")\n",
    "print(f\"  labels unique values: {torch.unique(sample['labels']).tolist()}\")\n",
    "print(\"âœ“ Dataset loading successful\")\n",
    "\n",
    "# ------------------ VISUALIZE MULTIPLE RANDOM SAMPLES ------------------\n",
    "\n",
    "num_show = 4  # how many examples to show\n",
    "indices = random.sample(range(len(train_dataset)), k=min(num_show, len(train_dataset)))\n",
    "\n",
    "fig, axes = plt.subplots(len(indices), 2, figsize=(10, 4 * len(indices)))\n",
    "\n",
    "# Ensure axes is 2D [row, col]\n",
    "if len(indices) == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for row, idx in enumerate(indices):\n",
    "    s = train_dataset[idx]\n",
    "    img_tensor = s[\"pixel_values\"]          # (3, H, W)\n",
    "    mask_tensor = s[\"labels\"]               # (H, W)\n",
    "\n",
    "    # Convert image back to [0,1] for visualization\n",
    "    img_np = img_tensor.numpy().transpose(1, 2, 0)\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)\n",
    "\n",
    "    # Show image\n",
    "    axes[row, 0].imshow(img_np)\n",
    "    axes[row, 0].set_title(f\"Image (idx={idx})\", fontsize=12, fontweight='bold')\n",
    "    axes[row, 0].axis('off')\n",
    "\n",
    "    # Show mask with custom colormap\n",
    "    axes[row, 1].imshow(mask_tensor.numpy(), cmap=class_cmap, vmin=0, vmax=num_classes-1)\n",
    "    axes[row, 1].set_title(\"Mask (0:bg,1:ut,2:tube,3:ov)\", fontsize=12, fontweight='bold')\n",
    "    axes[row, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148ba0a",
   "metadata": {},
   "source": [
    "## 11. Metrics\n",
    "\n",
    "Define IoU and Dice coefficient metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(preds, labels, num_classes):\n",
    "    \"\"\"Compute mean IoU across all classes\"\"\"\n",
    "    ious = []\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (preds == cls)\n",
    "        label_mask = (labels == cls)\n",
    "        \n",
    "        intersection = np.logical_and(pred_mask, label_mask).sum()\n",
    "        union = np.logical_or(pred_mask, label_mask).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = float('nan')\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        ious.append(iou)\n",
    "    \n",
    "    return np.nanmean(ious)\n",
    "\n",
    "def compute_dice(preds, labels, num_classes):\n",
    "    \"\"\"Compute mean Dice coefficient\"\"\"\n",
    "    dices = []\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (preds == cls)\n",
    "        label_mask = (labels == cls)\n",
    "        \n",
    "        intersection = np.logical_and(pred_mask, label_mask).sum()\n",
    "        dice = (2. * intersection) / (pred_mask.sum() + label_mask.sum() + 1e-8)\n",
    "        dices.append(dice)\n",
    "    \n",
    "    return np.mean(dices)\n",
    "\n",
    "def compute_per_class_metrics(preds, labels, num_classes):\n",
    "    \"\"\"Compute IoU and Dice for each class\"\"\"\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    metrics = {}\n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (preds == cls)\n",
    "        label_mask = (labels == cls)\n",
    "        \n",
    "        intersection = np.logical_and(pred_mask, label_mask).sum()\n",
    "        union = np.logical_or(pred_mask, label_mask).sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "            dice = (2. * intersection) / (pred_mask.sum() + label_mask.sum() + 1e-8)\n",
    "        else:\n",
    "            iou = float('nan')\n",
    "            dice = float('nan')\n",
    "        \n",
    "        metrics[f\"class_{cls}_iou\"] = iou\n",
    "        metrics[f\"class_{cls}_dice\"] = dice\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"âœ“ Metric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdec1e",
   "metadata": {},
   "source": [
    "## 12. Training Configuration\n",
    "\n",
    "Set up training hyperparameters and arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./segformer-gynsurg-b0\"\n",
    "BATCH_SIZE = 4  # Reduce to 2 if OOM\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image size: {TARGET_SIZE}x{TARGET_SIZE}\")\n",
    "print(f\"  FP16 training: {torch.cuda.is_available()}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e4c75",
   "metadata": {},
   "source": [
    "## 13. Custom Trainer\n",
    "\n",
    "Custom trainer with IoU/Dice metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3016d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with IoU/Dice metrics\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Compute loss\"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Upsample logits to match label size\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(upsampled_logits, labels, ignore_index=-100)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, \n",
    "                       ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"Custom evaluation with IoU and Dice\"\"\"\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_iou = 0\n",
    "        total_dice = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Per-class metrics\n",
    "        per_class_iou = np.zeros(num_classes)\n",
    "        per_class_dice = np.zeros(num_classes)\n",
    "        per_class_count = np.zeros(num_classes)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=description):\n",
    "                pixel_values = batch[\"pixel_values\"].to(model.device)\n",
    "                labels = batch[\"labels\"].to(model.device)\n",
    "                \n",
    "                outputs = model(pixel_values=pixel_values)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Upsample and compute loss\n",
    "                upsampled_logits = F.interpolate(\n",
    "                    logits,\n",
    "                    size=labels.shape[-2:],\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False\n",
    "                )\n",
    "                loss = F.cross_entropy(upsampled_logits, labels, ignore_index=-100)\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = upsampled_logits.argmax(dim=1)\n",
    "                \n",
    "                # Compute overall metrics\n",
    "                iou = compute_iou(preds, labels, num_classes)\n",
    "                dice = compute_dice(preds, labels, num_classes)\n",
    "                \n",
    "                # Compute per-class metrics\n",
    "                class_metrics = compute_per_class_metrics(preds, labels, num_classes)\n",
    "                for cls in range(num_classes):\n",
    "                    iou_val = class_metrics[f\"class_{cls}_iou\"]\n",
    "                    dice_val = class_metrics[f\"class_{cls}_dice\"]\n",
    "                    if not np.isnan(iou_val):\n",
    "                        per_class_iou[cls] += iou_val\n",
    "                        per_class_dice[cls] += dice_val\n",
    "                        per_class_count[cls] += 1\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_iou += iou\n",
    "                total_dice += dice\n",
    "                num_batches += 1\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        metrics = {\n",
    "            f\"{metric_key_prefix}_loss\": total_loss / num_batches,\n",
    "            f\"{metric_key_prefix}_iou\": total_iou / num_batches,\n",
    "            f\"{metric_key_prefix}_dice\": total_dice / num_batches,\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        for cls in range(num_classes):\n",
    "            if per_class_count[cls] > 0:\n",
    "                metrics[f\"{metric_key_prefix}_iou_{id2label[cls]}\"] = per_class_iou[cls] / per_class_count[cls]\n",
    "                metrics[f\"{metric_key_prefix}_dice_{id2label[cls]}\"] = per_class_dice[cls] / per_class_count[cls]\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"âœ“ Custom trainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c1c4e",
   "metadata": {},
   "source": [
    "## 14. Train Model\n",
    "\n",
    "Start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SegmentationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training on {len(train_dataset)} samples\")\n",
    "print(f\"Validating on {len(val_dataset)} samples\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27846bb3",
   "metadata": {},
   "source": [
    "## 15. Save Model\n",
    "\n",
    "Save the trained model and processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ“ Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ccf3b2",
   "metadata": {},
   "source": [
    "## 16. Inference and Visualization\n",
    "\n",
    "Visualize model predictions on validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18369737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, processor, num_samples=5, indices=None):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    if indices is None:\n",
    "        indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    else:\n",
    "        num_samples = len(indices)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(20, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, sample_idx in enumerate(indices):\n",
    "            sample = dataset[sample_idx]\n",
    "            pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "            true_mask = sample[\"labels\"]\n",
    "            \n",
    "            # Get prediction\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Upsample to original size\n",
    "            upsampled_logits = F.interpolate(\n",
    "                logits,\n",
    "                size=true_mask.shape,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "            pred_mask = upsampled_logits.argmax(dim=1).squeeze().cpu()\n",
    "            \n",
    "            # Denormalize image for visualization\n",
    "            img = pixel_values.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            # Compute metrics for this sample\n",
    "            sample_iou = compute_iou(pred_mask.unsqueeze(0), true_mask.unsqueeze(0), num_classes)\n",
    "            sample_dice = compute_dice(pred_mask.unsqueeze(0), true_mask.unsqueeze(0), num_classes)\n",
    "            \n",
    "            # Plot input image\n",
    "            axes[idx, 0].imshow(img)\n",
    "            axes[idx, 0].set_title(\"Input Image\", fontsize=12, fontweight='bold')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            # Plot ground truth\n",
    "            axes[idx, 1].imshow(true_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "            axes[idx, 1].set_title(\"Ground Truth\", fontsize=12, fontweight='bold')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            # Plot prediction\n",
    "            axes[idx, 2].imshow(pred_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "            axes[idx, 2].set_title(f\"Prediction\\nIoU: {sample_iou:.3f}, Dice: {sample_dice:.3f}\", \n",
    "                                  fontsize=12, fontweight='bold')\n",
    "            axes[idx, 2].axis('off')\n",
    "            \n",
    "            # Plot overlay\n",
    "            img_uint8 = (img * 255).astype(np.uint8)\n",
    "            overlay = img_uint8.copy()\n",
    "            colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n",
    "            \n",
    "            for cls in range(1, num_classes):  # Skip background\n",
    "                mask_bool = pred_mask.numpy() == cls\n",
    "                if np.any(mask_bool):\n",
    "                    color = colors[cls][:3] * 255\n",
    "                    overlay[mask_bool] = (overlay[mask_bool] * 0.4 + color * 0.6).astype(np.uint8)\n",
    "            \n",
    "            axes[idx, 3].imshow(overlay)\n",
    "            axes[idx, 3].set_title(\"Overlay\", fontsize=12, fontweight='bold')\n",
    "            axes[idx, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZING PREDICTIONS ON VALIDATION SET\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "visualize_predictions(model, val_dataset, processor, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d73280",
   "metadata": {},
   "source": [
    "## 17. Final Evaluation Metrics\n",
    "\n",
    "Compute comprehensive metrics on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON VALIDATION SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "total_iou = 0\n",
    "total_dice = 0\n",
    "per_class_iou = np.zeros(num_classes)\n",
    "per_class_dice = np.zeros(num_classes)\n",
    "per_class_count = np.zeros(num_classes)\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        preds = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        # Overall metrics\n",
    "        iou = compute_iou(preds, labels, num_classes)\n",
    "        dice = compute_dice(preds, labels, num_classes)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        class_metrics = compute_per_class_metrics(preds, labels, num_classes)\n",
    "        for cls in range(num_classes):\n",
    "            iou_val = class_metrics[f\"class_{cls}_iou\"]\n",
    "            dice_val = class_metrics[f\"class_{cls}_dice\"]\n",
    "            if not np.isnan(iou_val):\n",
    "                per_class_iou[cls] += iou_val\n",
    "                per_class_dice[cls] += dice_val\n",
    "                per_class_count[cls] += 1\n",
    "        \n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "        num_batches += 1\n",
    "\n",
    "mean_iou = total_iou / num_batches\n",
    "mean_dice = total_dice / num_batches\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OVERALL METRICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean IoU:  {mean_iou:.4f}\")\n",
    "print(f\"Mean Dice: {mean_dice:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PER-CLASS METRICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Class':<20} {'IoU':>10} {'Dice':>10} {'Samples':>10}\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "for cls in range(num_classes):\n",
    "    organ_name = id2label[cls]\n",
    "    if per_class_count[cls] > 0:\n",
    "        cls_iou = per_class_iou[cls] / per_class_count[cls]\n",
    "        cls_dice = per_class_dice[cls] / per_class_count[cls]\n",
    "        print(f\"{organ_name:<20} {cls_iou:>10.4f} {cls_dice:>10.4f} {int(per_class_count[cls]):>10}\")\n",
    "    else:\n",
    "        print(f\"{organ_name:<20} {'N/A':>10} {'N/A':>10} {'0':>10}\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a49ec1",
   "metadata": {},
   "source": [
    "## 18. Create Prediction Function for Inference\n",
    "\n",
    "Helper functions for single image inference and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path, model, processor, device='cuda'):\n",
    "    \"\"\"\n",
    "    Predict segmentation mask for a single image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        model: Trained SegFormer model\n",
    "        processor: SegFormer image processor\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        pred_mask: Predicted segmentation mask (numpy array)\n",
    "        confidence: Confidence scores per pixel\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        confidence, pred_mask = torch.max(probs, dim=1)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        pred_mask = F.interpolate(\n",
    "            pred_mask.unsqueeze(1).float(),\n",
    "            size=image.size[::-1],\n",
    "            mode=\"nearest\"\n",
    "        ).squeeze().cpu().numpy()\n",
    "        \n",
    "        confidence = F.interpolate(\n",
    "            confidence.unsqueeze(1),\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze().cpu().numpy()\n",
    "    \n",
    "    return pred_mask.astype(np.uint8), confidence\n",
    "\n",
    "def visualize_single_prediction(image_path, pred_mask, confidence=None):\n",
    "    \"\"\"Visualize prediction for a single image\"\"\"\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3 if confidence is None else 4, figsize=(20, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Predicted mask\n",
    "    axes[1].imshow(pred_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "    axes[1].set_title(\"Predicted Mask\", fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = image.copy().astype(float)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n",
    "    \n",
    "    for cls in range(1, num_classes):\n",
    "        mask_bool = pred_mask == cls\n",
    "        if np.any(mask_bool):\n",
    "            color = colors[cls][:3] * 255\n",
    "            overlay[mask_bool] = overlay[mask_bool] * 0.4 + color * 0.6\n",
    "    \n",
    "    axes[2].imshow(overlay.astype(np.uint8))\n",
    "    axes[2].set_title(\"Overlay\", fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Confidence map\n",
    "    if confidence is not None:\n",
    "        im = axes[3].imshow(confidence, cmap='viridis', vmin=0, vmax=1)\n",
    "        axes[3].set_title(\"Confidence Map\", fontsize=14, fontweight='bold')\n",
    "        axes[3].axis('off')\n",
    "        plt.colorbar(im, ax=axes[3], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ Inference functions created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  pred_mask, confidence = predict_single_image('/path/to/image.png', model, processor)\")\n",
    "print(\"  visualize_single_prediction('/path/to/image.png', pred_mask, confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85403de4",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Model Training Complete!\n",
    "\n",
    "### ðŸ“Š Results Summary\n",
    "- Model trained on gynecological surgery frames\n",
    "- Segments 4 classes: background, uterus, ovary, fallopian_tube\n",
    "- Metrics: IoU and Dice coefficient computed per class\n",
    "\n",
    "### ðŸš€ Next Steps for Hackathon\n",
    "\n",
    "#### Quick Improvements:\n",
    "1. **Try SegFormer-B1**: Better accuracy with slightly more parameters\n",
    "   ```python\n",
    "   MODEL_NAME = \"nvidia/segformer-b1-finetuned-ade-512-512\"\n",
    "   ```\n",
    "\n",
    "2. **Increase training epochs**: Try 75-100 epochs\n",
    "3. **Adjust learning rate**: Try 3e-5 or 7e-5\n",
    "4. **Larger batch size**: If GPU allows, increase to 8 or 16\n",
    "\n",
    "#### Advanced Techniques:\n",
    "5. **Class weighting**: Handle class imbalance\n",
    "6. **Combined loss**: Dice + CE loss\n",
    "7. **Test-time augmentation**: Average predictions from augmented versions\n",
    "8. **Post-processing**: CRF or morphological operations\n",
    "9. **Ensemble**: Combine B0 and B1 predictions\n",
    "\n",
    "#### Demo/Presentation:\n",
    "10. **Video inference**: Process full surgical video sequences\n",
    "11. **Real-time demo**: Show live segmentation on video frames\n",
    "12. **Metrics dashboard**: Interactive visualization of performance\n",
    "13. **Comparison**: Show before/after or compare with baseline\n",
    "\n",
    "### ðŸ“ Saved Files\n",
    "- Model: `./segformer-gynsurg-b0/`\n",
    "- Includes: model weights, config, processor\n",
    "\n",
    "Good luck with your hackathon! ðŸš€ðŸ¥"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
