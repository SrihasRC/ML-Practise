{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68483b69",
   "metadata": {},
   "source": [
    "# üè• Gynecological Surgery Organ Segmentation with SegFormer\n",
    "\n",
    "This notebook trains a SegFormer model to segment organs (uterus, ovary, fallopian tube) from laparoscopic surgery videos.\n",
    "\n",
    "**Model**: SegFormer-B0 (Hugging Face)  \n",
    "**Dataset**: BlackWalkersAnatomy from Kaggle  \n",
    "**Classes**: Background, Uterus, Ovary, Fallopian Tube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c04a09",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies with compatible versions\n",
    "print(\"Installing dependencies with compatible versions...\")\n",
    "\n",
    "!pip install -q --no-cache-dir \\\n",
    "    \"numpy==1.26.4\" \\\n",
    "    \"transformers==4.44.0\" \\\n",
    "    \"datasets==2.14.0\" \\\n",
    "    \"albumentations\" \\\n",
    "    \"evaluate\" \\\n",
    "    \"Pillow\"\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"üî¥ CRITICAL: RESTART KERNEL NOW\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nColab: Runtime ‚Üí Restart runtime\")\n",
    "print(\"Kaggle: Click restart button (‚ü≥)\")\n",
    "print(\"\\nAfter restart:\")\n",
    "print(\"  ‚Ä¢ Skip this cell\")\n",
    "print(\"  ‚Ä¢ Run from Cell 2 onwards\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e15b54",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b29ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    SegformerForSemanticSegmentation, \n",
    "    SegformerImageProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e0293",
   "metadata": {},
   "source": [
    "## 3. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "BASE_PATH = \"/kaggle/input/blackwalkersanatomy/GynSurg_Anatomy_Dataset\"\n",
    "IMAGE_BASE = os.path.join(BASE_PATH, \"ganseg\")\n",
    "MASK_BASE = os.path.join(BASE_PATH, \"ganseg_mask\")\n",
    "\n",
    "# Class mapping for gynecological organs\n",
    "id2label = {\n",
    "    0: \"background\",\n",
    "    1: \"uterus\",\n",
    "    2: \"fallopian_tube\",\n",
    "    3: \"ovary\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_classes = len(id2label)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class mapping:\")\n",
    "for class_id, label in id2label.items():\n",
    "    print(f\"  Class {class_id}: {label}\")\n",
    "\n",
    "# Global variable for intensity mapping (will be set after data exploration)\n",
    "INTENSITY_TO_CLASS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17316364",
   "metadata": {},
   "source": [
    "## 4. Collect Image-Mask Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corresponding_frame(mask_path, image_base_dir):\n",
    "    \"\"\"\n",
    "    Find the corresponding frame image for a mask\n",
    "    \n",
    "    mask_path: /path/to/ganseg_mask/GANSEG_01/0.mp4_/0_010800_06-00-00_mask.png\n",
    "    Returns: /path/to/ganseg/GANSEG_01/0.mp4_/0_010800_06-00-00.png (or .jpg)\n",
    "    \"\"\"\n",
    "    mask_filename = os.path.basename(mask_path)\n",
    "    \n",
    "    # Get the directory structure\n",
    "    parts = Path(mask_path).parts\n",
    "    ganseg_idx = parts.index('ganseg_mask')\n",
    "    ganseg_id = parts[ganseg_idx + 1]  # e.g., GANSEG_01\n",
    "    video_folder = parts[ganseg_idx + 2]  # e.g., 0.mp4_\n",
    "    \n",
    "    # Construct image directory\n",
    "    image_dir = os.path.join(image_base_dir, ganseg_id, video_folder)\n",
    "    \n",
    "    # Remove '_mask' from filename to get image filename\n",
    "    image_filename = mask_filename.replace('_mask.png', '.png')\n",
    "    image_path = os.path.join(image_dir, image_filename)\n",
    "    \n",
    "    # Try different extensions if needed\n",
    "    if not os.path.exists(image_path):\n",
    "        image_path = image_path.replace('.png', '.jpg')\n",
    "    if not os.path.exists(image_path):\n",
    "        image_path = image_path.replace('.jpg', '.jpeg')\n",
    "    \n",
    "    return image_path if os.path.exists(image_path) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all mask files\n",
    "print(\"Collecting mask files from all GANSEG folders...\")\n",
    "all_mask_paths = []\n",
    "\n",
    "for ganseg_dir in sorted(os.listdir(MASK_BASE)):\n",
    "    ganseg_path = os.path.join(MASK_BASE, ganseg_dir)\n",
    "    if not os.path.isdir(ganseg_path):\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Processing {ganseg_dir}...\", end=\" \")\n",
    "    folder_mask_count = 0\n",
    "    \n",
    "    for video_folder in sorted(os.listdir(ganseg_path)):\n",
    "        video_path = os.path.join(ganseg_path, video_folder)\n",
    "        if not os.path.isdir(video_path):\n",
    "            continue\n",
    "        \n",
    "        # Get all mask files in this video folder\n",
    "        mask_files = glob.glob(os.path.join(video_path, '*_mask.png'))\n",
    "        all_mask_paths.extend(mask_files)\n",
    "        folder_mask_count += len(mask_files)\n",
    "    \n",
    "    print(f\"{folder_mask_count} masks found\")\n",
    "\n",
    "print(f\"\\nTotal mask files found: {len(all_mask_paths)}\")\n",
    "\n",
    "# Find corresponding images\n",
    "print(\"\\nMatching images to masks...\")\n",
    "image_paths = []\n",
    "mask_paths = []\n",
    "missing_count = 0\n",
    "\n",
    "for mask_path in tqdm(all_mask_paths):\n",
    "    image_path = find_corresponding_frame(mask_path, IMAGE_BASE)\n",
    "    if image_path:\n",
    "        image_paths.append(image_path)\n",
    "        mask_paths.append(mask_path)\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(f\"\\n‚úì Successfully matched {len(image_paths)} image-mask pairs\")\n",
    "if missing_count > 0:\n",
    "    print(f\"‚úó {missing_count} masks without corresponding images\")\n",
    "\n",
    "# Show sample paths\n",
    "if len(image_paths) > 0:\n",
    "    print(f\"\\nSample paths:\")\n",
    "    print(f\"  Image: {image_paths[0]}\")\n",
    "    print(f\"  Mask:  {mask_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bbc16a",
   "metadata": {},
   "source": [
    "## 5. Step 2: Create Intensity Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No image-mask pairs found! Check dataset structure.\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: ANALYZING MASK INTENSITIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze multiple masks to get consistent intensity values\n",
    "print(\"\\nAnalyzing first 10 masks to detect all intensity values...\")\n",
    "all_intensities = set()\n",
    "\n",
    "for i, mask_path in enumerate(mask_paths[:10]):\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "    if len(mask.shape) == 3:\n",
    "        mask = mask[:,:,0]  # Take first channel if RGB\n",
    "    all_intensities.update(np.unique(mask))\n",
    "\n",
    "all_intensities = sorted(list(all_intensities))\n",
    "print(f\"Found {len(all_intensities)} unique intensity values across samples: {all_intensities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a941589",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: CREATE INTENSITY TO CLASS MAPPING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create intensity to class mapping\n",
    "INTENSITY_TO_CLASS = {intensity: idx for idx, intensity in enumerate(all_intensities)}\n",
    "print(f\"\\nIntensity ‚Üí Class ID mapping:\")\n",
    "for intensity, class_id in INTENSITY_TO_CLASS.items():\n",
    "    organ_name = id2label.get(class_id, \"unknown\")\n",
    "    print(f\"  Intensity {intensity:3d} ‚Üí Class {class_id} ({organ_name})\")\n",
    "\n",
    "print(\"\\n‚úì Intensity mapping created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db77eb",
   "metadata": {},
   "source": [
    "## 6. Step 3: Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622fcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: VISUALIZING SAMPLE DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find a mask with multiple intensities for better visualization\n",
    "best_mask_idx = 0\n",
    "max_intensities = 0\n",
    "for i in range(min(20, len(mask_paths))):\n",
    "    mask = np.array(Image.open(mask_paths[i]))\n",
    "    if len(mask.shape) == 3:\n",
    "        mask = mask[:,:,0]\n",
    "    n_intensities = len(np.unique(mask))\n",
    "    if n_intensities > max_intensities:\n",
    "        max_intensities = n_intensities\n",
    "        best_mask_idx = i\n",
    "\n",
    "sample_img_path = image_paths[best_mask_idx]\n",
    "sample_mask_path = mask_paths[best_mask_idx]\n",
    "\n",
    "print(f\"Visualizing: {os.path.basename(sample_mask_path)}\")\n",
    "print(f\"This sample contains {max_intensities} classes\\n\")\n",
    "\n",
    "# Load image and mask\n",
    "img = Image.open(sample_img_path).convert('RGB')\n",
    "mask = Image.open(sample_mask_path)\n",
    "mask_array = np.array(mask)\n",
    "\n",
    "# Handle RGB masks\n",
    "if len(mask_array.shape) == 3:\n",
    "    mask_array = mask_array[:,:,0]\n",
    "\n",
    "unique_values = np.unique(mask_array)\n",
    "print(f\"Intensities present: {unique_values}\")\n",
    "\n",
    "# Pixel distribution\n",
    "print(f\"\\nPixel distribution:\")\n",
    "for val in unique_values:\n",
    "    count = np.sum(mask_array == val)\n",
    "    percentage = (count / mask_array.size) * 100\n",
    "    class_id = INTENSITY_TO_CLASS[val]\n",
    "    organ = id2label[class_id]\n",
    "    print(f\"  {organ:15s} (intensity {val:3d}): {count:7d} pixels ({percentage:5.2f}%)\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Row 1: Original, Full Mask, Overlay\n",
    "axes[0, 0].imshow(img)\n",
    "axes[0, 0].set_title(\"Original Surgical Image\", fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Map intensities to class IDs for visualization\n",
    "class_mask = np.zeros_like(mask_array)\n",
    "for intensity, class_id in INTENSITY_TO_CLASS.items():\n",
    "    class_mask[mask_array == intensity] = class_id\n",
    "\n",
    "axes[0, 1].imshow(class_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "axes[0, 1].set_title(\"Segmentation Mask\", fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Create overlay\n",
    "img_array = np.array(img)\n",
    "overlay = img_array.copy().astype(float)\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n",
    "for val in unique_values:\n",
    "    class_id = INTENSITY_TO_CLASS[val]\n",
    "    if class_id == 0:  # Skip background\n",
    "        continue\n",
    "    mask_bool = mask_array == val\n",
    "    color = colors[class_id][:3] * 255\n",
    "    overlay[mask_bool] = overlay[mask_bool] * 0.4 + color * 0.6\n",
    "\n",
    "axes[0, 2].imshow(overlay.astype(np.uint8))\n",
    "axes[0, 2].set_title(\"Overlay on Image\", fontsize=14, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Row 2: Individual classes\n",
    "class_colors_viz = {\n",
    "    0: [0, 0, 0],       # background - not shown\n",
    "    1: [255, 100, 100], # uterus - red\n",
    "    2: [100, 255, 100], # fallopian_tube - green\n",
    "    3: [100, 100, 255], # ovary - blue\n",
    "}\n",
    "\n",
    "for idx, (class_id, organ_name) in enumerate(id2label.items()):\n",
    "    if class_id == 0:  # Skip background\n",
    "        continue\n",
    "    \n",
    "    col_idx = class_id - 1\n",
    "    ax = axes[1, col_idx]\n",
    "    \n",
    "    # Find corresponding intensity\n",
    "    intensity_val = [k for k, v in INTENSITY_TO_CLASS.items() if v == class_id][0]\n",
    "    \n",
    "    # Create colored overlay for this class only\n",
    "    colored_img = img_array.copy()\n",
    "    mask_bool = mask_array == intensity_val\n",
    "    \n",
    "    if np.any(mask_bool):\n",
    "        color = np.array(class_colors_viz[class_id])\n",
    "        colored_img[mask_bool] = (colored_img[mask_bool] * 0.3 + color * 0.7).astype(np.uint8)\n",
    "    \n",
    "    ax.imshow(colored_img)\n",
    "    \n",
    "    pixel_count = np.sum(mask_array == intensity_val)\n",
    "    percentage = (pixel_count / mask_array.size) * 100\n",
    "    \n",
    "    ax.set_title(f\"{organ_name.upper()}\\n({pixel_count} pixels, {percentage:.1f}%)\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858a853",
   "metadata": {},
   "source": [
    "## 7. Prepare Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARING TRAIN-VALIDATION SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Group by GANSEG folder and video to avoid data leakage\n",
    "video_groups = []\n",
    "for mask_path in mask_paths:\n",
    "    parts = Path(mask_path).parts\n",
    "    ganseg_id = parts[parts.index('ganseg_mask') + 1]\n",
    "    video_folder = parts[parts.index('ganseg_mask') + 2]\n",
    "    video_groups.append(f\"{ganseg_id}_{video_folder}\")\n",
    "\n",
    "# Convert to dataframe for easier splitting\n",
    "df = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'mask_path': mask_paths,\n",
    "    'video_group': video_groups\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Total samples: {len(df)}\")\n",
    "print(f\"  Unique video groups: {df['video_group'].nunique()}\")\n",
    "\n",
    "print(f\"\\nSamples per video group:\")\n",
    "group_counts = df['video_group'].value_counts()\n",
    "for group, count in group_counts.items():\n",
    "    print(f\"  {group}: {count} frames\")\n",
    "\n",
    "# Split by video group to avoid leakage\n",
    "unique_groups = df['video_group'].unique()\n",
    "train_groups, val_groups = train_test_split(\n",
    "    unique_groups, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df['video_group'].isin(train_groups)]\n",
    "val_df = df[df['video_group'].isin(val_groups)]\n",
    "\n",
    "train_images = train_df['image_path'].tolist()\n",
    "train_masks = train_df['mask_path'].tolist()\n",
    "val_images = val_df['image_path'].tolist()\n",
    "val_masks = val_df['mask_path'].tolist()\n",
    "\n",
    "print(f\"\\n‚úì Split complete:\")\n",
    "print(f\"  Training samples: {len(train_images)} ({len(train_images)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Validation samples: {len(val_images)} ({len(val_images)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTraining video groups: {sorted(train_groups)}\")\n",
    "print(f\"Validation video groups: {sorted(val_groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446a090",
   "metadata": {},
   "source": [
    "## 8. Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GynSurgDataset(Dataset):\n",
    "    \"\"\"Dataset for gynecological surgery segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, processor, \n",
    "                 target_size=512, augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.processor = processor\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Augmentation pipeline for training\n",
    "        if augment:\n",
    "            self.transform = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.3),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, \n",
    "                                  rotate_limit=15, p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, \n",
    "                                    val_shift_limit=10, p=0.3),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = None\n",
    "    \n",
    "    def mask_to_class_ids(self, mask):\n",
    "        \"\"\"\n",
    "        Convert grayscale mask intensities to class IDs (0, 1, 2, 3).\n",
    "        Uses the global INTENSITY_TO_CLASS mapping.\n",
    "        \"\"\"\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        # Handle RGB masks by taking first channel\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask[:,:,0]\n",
    "        \n",
    "        # Create class ID mask\n",
    "        h, w = mask.shape\n",
    "        class_mask = np.zeros((h, w), dtype=np.int64)\n",
    "        \n",
    "        # Map each intensity to its class ID\n",
    "        for intensity, class_id in INTENSITY_TO_CLASS.items():\n",
    "            class_mask[mask == intensity] = class_id\n",
    "        \n",
    "        return class_mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Load and convert mask\n",
    "        mask = Image.open(self.mask_paths[idx])\n",
    "        mask = self.mask_to_class_ids(mask)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        \n",
    "        # Process image with SegFormer processor\n",
    "        encoded = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Resize mask to target size (nearest neighbor to preserve class IDs)\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float()\n",
    "        mask_resized = F.interpolate(\n",
    "            mask_tensor, \n",
    "            size=(self.target_size, self.target_size), \n",
    "            mode=\"nearest\"\n",
    "        )\n",
    "        mask_resized = mask_resized.squeeze().long()\n",
    "        \n",
    "        # Prepare output\n",
    "        encoded_inputs = {k: v.squeeze(0) for k, v in encoded.items()}\n",
    "        encoded_inputs[\"labels\"] = mask_resized\n",
    "        \n",
    "        return encoded_inputs\n",
    "\n",
    "print(\"‚úì Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314af4b",
   "metadata": {},
   "source": [
    "## 9. Initialize Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "# For better accuracy, try: \"nvidia/segformer-b1-finetuned-ade-512-512\"\n",
    "\n",
    "print(\"Loading SegFormer model and processor...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "\n",
    "# Initialize processor\n",
    "processor = SegformerImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Initialize model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úì Model loaded successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cfba2f",
   "metadata": {},
   "source": [
    "## 10. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43740d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = 512\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "train_dataset = GynSurgDataset(\n",
    "    train_images, \n",
    "    train_masks, \n",
    "    processor,\n",
    "    target_size=TARGET_SIZE,\n",
    "    augment=True  # Enable augmentation for training\n",
    ")\n",
    "\n",
    "val_dataset = GynSurgDataset(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    processor,\n",
    "    target_size=TARGET_SIZE,\n",
    "    augment=False  # No augmentation for validation\n",
    ")\n",
    "\n",
    "print(f\"‚úì Datasets created:\")\n",
    "print(f\"  Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"  Val dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Test dataset loading\n",
    "print(\"\\nTesting dataset loading...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"  labels shape: {sample['labels'].shape}\")\n",
    "print(f\"  labels unique values: {torch.unique(sample['labels']).tolist()}\")\n",
    "print(\"‚úì Dataset loading successful\")\n",
    "\n",
    "# Visualize a preprocessed sample\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Denormalize image for visualization\n",
    "img = sample['pixel_values'].numpy().transpose(1, 2, 0)\n",
    "img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Preprocessed Image (512x512)\", fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sample['labels'].numpy(), cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "axes[1].set_title(\"Preprocessed Mask (512x512)\", fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c399a4c",
   "metadata": {},
   "source": [
    "## 11. Define Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(preds, labels, num_classes):\n",
    "    \"\"\"Compute mean IoU across all classes\"\"\"\n",
    "    ious = []\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (preds == cls)\n",
    "        label_mask = (labels == cls)\n",
    "        \n",
    "        intersection = np.logical_and(pred_mask, label_mask).sum()\n",
    "        union = np.logical_or(pred_mask, label_mask).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = float('nan')\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        ious.append(iou)\n",
    "    \n",
    "    return np.nanmean(ious)\n",
    "\n",
    "def compute_dice(preds, labels, num_classes):\n",
    "    \"\"\"Compute mean Dice coefficient\"\"\"\n",
    "    dices = []\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (preds == cls)\n",
    "        label_mask = (labels == cls)\n",
    "        \n",
    "        intersection = np.logical_and(pred_mask, label_mask).sum()\n",
    "        dice = (2. * intersection) / (pred_mask.sum() + label_mask.sum() + 1e-8)\n",
    "        dices.append(dice)\n",
    "    \n",
    "    return np.mean(dices)\n",
    "\n",
    "def compute_per_class_metrics(preds, labels, num_classes):\n",
    "    \"\"\"Compute IoU and Dice for each class\"\"\"\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    metrics = {}\n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (preds == cls)\n",
    "        label_mask = (labels == cls)\n",
    "        \n",
    "        intersection = np.logical_and(pred_mask, label_mask).sum()\n",
    "        union = np.logical_or(pred_mask, label_mask).sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "            dice = (2. * intersection) / (pred_mask.sum() + label_mask.sum() + 1e-8)\n",
    "        else:\n",
    "            iou = float('nan')\n",
    "            dice = float('nan')\n",
    "        \n",
    "        metrics[f\"class_{cls}_iou\"] = iou\n",
    "        metrics[f\"class_{cls}_dice\"] = dice\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úì Metric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad8d20",
   "metadata": {},
   "source": [
    "## 12. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bc95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./segformer-gynsurg-b0\"\n",
    "BATCH_SIZE = 4  # Reduce to 2 if OOM\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image size: {TARGET_SIZE}x{TARGET_SIZE}\")\n",
    "print(f\"  FP16 training: {torch.cuda.is_available()}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b429f9",
   "metadata": {},
   "source": [
    "## 13. Custom Trainer with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with IoU/Dice metrics\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Compute loss\"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Upsample logits to match label size\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(upsampled_logits, labels, ignore_index=-100)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, \n",
    "                       ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"Custom evaluation with IoU and Dice\"\"\"\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_iou = 0\n",
    "        total_dice = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Per-class metrics\n",
    "        per_class_iou = np.zeros(num_classes)\n",
    "        per_class_dice = np.zeros(num_classes)\n",
    "        per_class_count = np.zeros(num_classes)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=description):\n",
    "                pixel_values = batch[\"pixel_values\"].to(model.device)\n",
    "                labels = batch[\"labels\"].to(model.device)\n",
    "                \n",
    "                outputs = model(pixel_values=pixel_values)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Upsample and compute loss\n",
    "                upsampled_logits = F.interpolate(\n",
    "                    logits,\n",
    "                    size=labels.shape[-2:],\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False\n",
    "                )\n",
    "                loss = F.cross_entropy(upsampled_logits, labels, ignore_index=-100)\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = upsampled_logits.argmax(dim=1)\n",
    "                \n",
    "                # Compute overall metrics\n",
    "                iou = compute_iou(preds, labels, num_classes)\n",
    "                dice = compute_dice(preds, labels, num_classes)\n",
    "                \n",
    "                # Compute per-class metrics\n",
    "                class_metrics = compute_per_class_metrics(preds, labels, num_classes)\n",
    "                for cls in range(num_classes):\n",
    "                    iou_val = class_metrics[f\"class_{cls}_iou\"]\n",
    "                    dice_val = class_metrics[f\"class_{cls}_dice\"]\n",
    "                    if not np.isnan(iou_val):\n",
    "                        per_class_iou[cls] += iou_val\n",
    "                        per_class_dice[cls] += dice_val\n",
    "                        per_class_count[cls] += 1\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_iou += iou\n",
    "                total_dice += dice\n",
    "                num_batches += 1\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        metrics = {\n",
    "            f\"{metric_key_prefix}_loss\": total_loss / num_batches,\n",
    "            f\"{metric_key_prefix}_iou\": total_iou / num_batches,\n",
    "            f\"{metric_key_prefix}_dice\": total_dice / num_batches,\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        for cls in range(num_classes):\n",
    "            if per_class_count[cls] > 0:\n",
    "                metrics[f\"{metric_key_prefix}_iou_{id2label[cls]}\"] = per_class_iou[cls] / per_class_count[cls]\n",
    "                metrics[f\"{metric_key_prefix}_dice_{id2label[cls]}\"] = per_class_dice[cls] / per_class_count[cls]\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"‚úì Custom trainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b0b00",
   "metadata": {},
   "source": [
    "## 14. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SegmentationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training on {len(train_dataset)} samples\")\n",
    "print(f\"Validating on {len(val_dataset)} samples\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21cf0c",
   "metadata": {},
   "source": [
    "## 15. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"‚úì Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6ae14",
   "metadata": {},
   "source": [
    "## 16. Inference and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, processor, num_samples=5, indices=None):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    if indices is None:\n",
    "        indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    else:\n",
    "        num_samples = len(indices)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(20, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, sample_idx in enumerate(indices):\n",
    "            sample = dataset[sample_idx]\n",
    "            pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "            true_mask = sample[\"labels\"]\n",
    "            \n",
    "            # Get prediction\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Upsample to original size\n",
    "            upsampled_logits = F.interpolate(\n",
    "                logits,\n",
    "                size=true_mask.shape,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "            pred_mask = upsampled_logits.argmax(dim=1).squeeze().cpu()\n",
    "            \n",
    "            # Denormalize image for visualization\n",
    "            img = pixel_values.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            # Compute metrics for this sample\n",
    "            sample_iou = compute_iou(pred_mask.unsqueeze(0), true_mask.unsqueeze(0), num_classes)\n",
    "            sample_dice = compute_dice(pred_mask.unsqueeze(0), true_mask.unsqueeze(0), num_classes)\n",
    "            \n",
    "            # Plot input image\n",
    "            axes[idx, 0].imshow(img)\n",
    "            axes[idx, 0].set_title(\"Input Image\", fontsize=12, fontweight='bold')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            # Plot ground truth\n",
    "            axes[idx, 1].imshow(true_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "            axes[idx, 1].set_title(\"Ground Truth\", fontsize=12, fontweight='bold')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            # Plot prediction\n",
    "            axes[idx, 2].imshow(pred_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "            axes[idx, 2].set_title(f\"Prediction\\nIoU: {sample_iou:.3f}, Dice: {sample_dice:.3f}\", \n",
    "                                  fontsize=12, fontweight='bold')\n",
    "            axes[idx, 2].axis('off')\n",
    "            \n",
    "            # Plot overlay\n",
    "            img_uint8 = (img * 255).astype(np.uint8)\n",
    "            overlay = img_uint8.copy()\n",
    "            colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n",
    "            \n",
    "            for cls in range(1, num_classes):  # Skip background\n",
    "                mask_bool = pred_mask.numpy() == cls\n",
    "                if np.any(mask_bool):\n",
    "                    color = colors[cls][:3] * 255\n",
    "                    overlay[mask_bool] = (overlay[mask_bool] * 0.4 + color * 0.6).astype(np.uint8)\n",
    "            \n",
    "            axes[idx, 3].imshow(overlay)\n",
    "            axes[idx, 3].set_title(\"Overlay\", fontsize=12, fontweight='bold')\n",
    "            axes[idx, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZING PREDICTIONS ON VALIDATION SET\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "visualize_predictions(model, val_dataset, processor, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c8ae8",
   "metadata": {},
   "source": [
    "## 17. Final Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8139f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON VALIDATION SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "total_iou = 0\n",
    "total_dice = 0\n",
    "per_class_iou = np.zeros(num_classes)\n",
    "per_class_dice = np.zeros(num_classes)\n",
    "per_class_count = np.zeros(num_classes)\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        preds = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        # Overall metrics\n",
    "        iou = compute_iou(preds, labels, num_classes)\n",
    "        dice = compute_dice(preds, labels, num_classes)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        class_metrics = compute_per_class_metrics(preds, labels, num_classes)\n",
    "        for cls in range(num_classes):\n",
    "            iou_val = class_metrics[f\"class_{cls}_iou\"]\n",
    "            dice_val = class_metrics[f\"class_{cls}_dice\"]\n",
    "            if not np.isnan(iou_val):\n",
    "                per_class_iou[cls] += iou_val\n",
    "                per_class_dice[cls] += dice_val\n",
    "                per_class_count[cls] += 1\n",
    "        \n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "        num_batches += 1\n",
    "\n",
    "mean_iou = total_iou / num_batches\n",
    "mean_dice = total_dice / num_batches\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OVERALL METRICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean IoU:  {mean_iou:.4f}\")\n",
    "print(f\"Mean Dice: {mean_dice:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PER-CLASS METRICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Class':<20} {'IoU':>10} {'Dice':>10} {'Samples':>10}\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "for cls in range(num_classes):\n",
    "    organ_name = id2label[cls]\n",
    "    if per_class_count[cls] > 0:\n",
    "        cls_iou = per_class_iou[cls] / per_class_count[cls]\n",
    "        cls_dice = per_class_dice[cls] / per_class_count[cls]\n",
    "        print(f\"{organ_name:<20} {cls_iou:>10.4f} {cls_dice:>10.4f} {int(per_class_count[cls]):>10}\")\n",
    "    else:\n",
    "        print(f\"{organ_name:<20} {'N/A':>10} {'N/A':>10} {'0':>10}\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a789e",
   "metadata": {},
   "source": [
    "## 18. Prediction Utilities for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ccab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path, model, processor, device='cuda'):\n",
    "    \"\"\"\n",
    "    Predict segmentation mask for a single image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        model: Trained SegFormer model\n",
    "        processor: SegFormer image processor\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        pred_mask: Predicted segmentation mask (numpy array)\n",
    "        confidence: Confidence scores per pixel\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        confidence, pred_mask = torch.max(probs, dim=1)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        pred_mask = F.interpolate(\n",
    "            pred_mask.unsqueeze(1).float(),\n",
    "            size=image.size[::-1],\n",
    "            mode=\"nearest\"\n",
    "        ).squeeze().cpu().numpy()\n",
    "        \n",
    "        confidence = F.interpolate(\n",
    "            confidence.unsqueeze(1),\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze().cpu().numpy()\n",
    "    \n",
    "    return pred_mask.astype(np.uint8), confidence\n",
    "\n",
    "def visualize_single_prediction(image_path, pred_mask, confidence=None):\n",
    "    \"\"\"Visualize prediction for a single image\"\"\"\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3 if confidence is None else 4, figsize=(20, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Predicted mask\n",
    "    axes[1].imshow(pred_mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "    axes[1].set_title(\"Predicted Mask\", fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = image.copy().astype(float)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n",
    "    \n",
    "    for cls in range(1, num_classes):\n",
    "        mask_bool = pred_mask == cls\n",
    "        if np.any(mask_bool):\n",
    "            color = colors[cls][:3] * 255\n",
    "            overlay[mask_bool] = overlay[mask_bool] * 0.4 + color * 0.6\n",
    "    \n",
    "    axes[2].imshow(overlay.astype(np.uint8))\n",
    "    axes[2].set_title(\"Overlay\", fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Confidence map\n",
    "    if confidence is not None:\n",
    "        im = axes[3].imshow(confidence, cmap='viridis', vmin=0, vmax=1)\n",
    "        axes[3].set_title(\"Confidence Map\", fontsize=14, fontweight='bold')\n",
    "        axes[3].axis('off')\n",
    "        plt.colorbar(im, ax=axes[3], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Inference functions created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  pred_mask, confidence = predict_single_image('/path/to/image.png', model, processor)\")\n",
    "print(\"  visualize_single_prediction('/path/to/image.png', pred_mask, confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5f63b",
   "metadata": {},
   "source": [
    "## üéØ Model Training Complete!\n",
    "\n",
    "### üìä Results Summary\n",
    "- Model trained on gynecological surgery frames\n",
    "- Segments 4 classes: background, uterus, ovary, fallopian_tube\n",
    "- Metrics: IoU and Dice coefficient computed per class\n",
    "\n",
    "### üöÄ Next Steps for Improvement\n",
    "\n",
    "#### Quick Improvements:\n",
    "1. **Try SegFormer-B1**: Better accuracy with slightly more parameters\n",
    "   ```python\n",
    "   MODEL_NAME = \"nvidia/segformer-b1-finetuned-ade-512-512\"\n",
    "   ```\n",
    "\n",
    "2. **Increase training epochs**: Try 75-100 epochs\n",
    "3. **Adjust learning rate**: Try 3e-5 or 7e-5\n",
    "4. **Larger batch size**: If GPU allows, increase to 8 or 16\n",
    "\n",
    "#### Advanced Techniques:\n",
    "5. **Class weighting**: Handle class imbalance\n",
    "6. **Combined loss**: Dice + CE loss\n",
    "7. **Test-time augmentation**: Average predictions from augmented versions\n",
    "8. **Post-processing**: CRF or morphological operations\n",
    "9. **Ensemble**: Combine B0 and B1 predictions\n",
    "\n",
    "#### Demo/Presentation:\n",
    "10. **Video inference**: Process full surgical video sequences\n",
    "11. **Real-time demo**: Show live segmentation on video frames\n",
    "12. **Metrics dashboard**: Interactive visualization of performance\n",
    "13. **Comparison**: Show before/after or compare with baseline\n",
    "\n",
    "### üìÅ Saved Files\n",
    "- Model: `./segformer-gynsurg-b0/`\n",
    "- Includes: model weights, config, processor\n",
    "\n",
    "Good luck with your project! üöÄüè•"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
